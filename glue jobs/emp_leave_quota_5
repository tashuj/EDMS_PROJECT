import sys
import boto3
from awsglue.transforms import *
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import *
from awsglue.utils import getResolvedOptions
from pyspark.sql.window import *
from pyspark.sql.types import *
import psycopg2

# Initialize Glue job
args = getResolvedOptions(sys.argv, [
    'JOB_NAME',
    'db_user',
    'db_pass',
    'db_host',
    'pg_url',
    'curr_ts',
    'start_date',
    'end_of_year',
    'start_of_year',
    'today',
    'CURRENT_YEAR'
])

# Accessing the arguments
db_user = args['db_user']
db_pass = args['db_pass']
db_host = args['db_host']
pg_url = args['pg_url']
curr_ts = args['curr_ts']
start_date = args['start_date']
end_of_year = args['end_of_year']
start_of_year = args['start_of_year']
today = args['today']
CURRENT_YEAR = args['CURRENT_YEAR']

# Example of logging the received arguments
print(f"Received arguments: {args}")

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

bucket_name = "poc-bootcamp-capstone-group4"
unprocessed_prefix = "bronze/leave_quota/unprocessed/"
processed_prefix = "bronze/leave_quota/processed/"

# Load existing data from PostgreSQL
pg_url = pg_url
pg_properties = {
    "user": db_user,
    "password": db_pass,
    "driver": "org.postgresql.Driver"
}
table_name = "emp_time_data"
table_name2 = "emp_leave_quota"

# Connect to PostgreSQL with psycopg2
conn = psycopg2.connect(
    dbname="postgres_capstone",
    user=db_user,
    password=db_pass,
    host=db_host,
    port="5432"
)
cur = conn.cursor()

schema = StructType([
    StructField("emp_id", LongType(), True),
    StructField("leave_quota", IntegerType(), True),
    StructField("year", IntegerType(), True)
])
df_new_raw = spark.read.schema(schema).csv(f"s3://{bucket_name}/{unprocessed_prefix}", header=True)
print(df_new_raw.show())

s3 = boto3.client('s3')
prefix = "bronze/leave_quota/unprocessed/"

response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)

if 'Contents' not in response or all(obj['Key'].endswith('/') for obj in response['Contents']):
    print("No files found in the bronze/unprocessed path. Exiting the job.")
    job.commit()
    
else:
    try:
        emp_time_df = spark.read.jdbc(url=pg_url, table=table_name, properties=pg_properties)
        print("data fetch from db successfuly")
    except:
        print("failed to fetch db")
    
    active_emp_ids_df = emp_time_df.filter(col("status") == "ACTIVE").select("emp_id").distinct()
    
    df_new = df_new_raw.filter(
        (col("leave_quota") >= 0) &
        (col("year").between(2000, 2100))
    )
    
    # Step 2: Use left semi join to filter df without bringing in columns from emp_time_df
    df_new = df_new.join(active_emp_ids_df, "emp_id", 'left_semi')
    
    normal_df_existing = spark.read.jdbc(url=pg_url, table=table_name2, properties=pg_properties)
    
    try:
        normal_df_existing.write \
            .jdbc(url=pg_url, table="emp_leave_quota_backup", mode="append", properties=pg_properties)
        
        print("emp_leave_quota_backup write successful.")
    
    except Exception as e:
        print("Write failed:", e)
    
    try:
        df_existing = spark.read.jdbc(url=pg_url, table="emp_leave_quota_backup", properties=pg_properties)
        print("naman this is an existing psql db",df_existing.show())
        df_combined = df_existing.unionByName(df_new)
        
    except Exception as e:
        print("No historical data found or error reading:", str(e))
        df_combined = df_new
    
    # Deduplicate on (emp_id, year)
    window_spec = Window.partitionBy("emp_id", "year").orderBy(col("leave_quota").desc())
    df_final = df_combined.withColumn("row_num", row_number().over(window_spec)) \
                          .filter(col("row_num") == 1) \
                          .drop("row_num")
    
    try:
        if df_final.rdd.isEmpty():
            print("Transformed DataFrame is empty. Restoring original data from backup.")
            # Write backup data back
            normal_df_existing.write \
                .format("jdbc") \
                .option("url", pg_url) \
                .option("dbtable", "emp_leave_quota") \
                .option("user", db_user) \
                .option("password", db_pass) \
                .option("driver", "org.postgresql.Driver") \
                .mode("overwrite") \
                .save()
        else:
            print("Writing transformed data to df_existing...")
            df_final.write \
                .format("jdbc") \
                .option("url", pg_url) \
                .option("dbtable", "emp_leave_quota") \
                .option("user", db_user) \
                .option("password", db_pass) \
                .option("driver", "org.postgresql.Driver") \
                .mode("overwrite") \
                .save()
    
        # To truncate emp_timeframe_df_backup using Spark
        cur.execute("TRUNCATE TABLE emp_leave_quota_backup;")
        conn.commit()
        cur.close()
        conn.close()
    
        print("emp_timeframe_df_backup truncated.")
        
    except Exception as e:
        print("Write failed:", e)
        cur.close()
        conn.close()
    
    
    # Move processed files
    s3 = boto3.client('s3')
    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=unprocessed_prefix)
    
    if 'Contents' in response:
        for obj in response['Contents']:
            source_key = obj['Key']
            if source_key.endswith('/'):
                continue
            file_name = source_key.split('/')[-1]
            destination_key = f"{processed_prefix}{file_name}"
    
            # Copy to processed folder
            s3.copy_object(
                Bucket=bucket_name,
                CopySource={'Bucket': bucket_name, 'Key': source_key},
                Key=destination_key
            )
    
            # Delete original file
            s3.delete_object(Bucket=bucket_name, Key=source_key)
            print(f"Moved {source_key} to {destination_key}")
    else:
        print("No new files found to process.")
    
    job.commit()
